{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import NoReturn\n",
    "\n",
    "from arguments import DataTrainingArguments, ModelArguments\n",
    "from datasets import DatasetDict, load_from_disk, load_metric, load_dataset,concatenate_datasets\n",
    "from trainer_qa import QuestionAnsweringTrainer\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_qa import check_no_error, postprocess_qa_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_from_disk('/opt/ml/input/data/train_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = datasets[\"train\"].column_names\n",
    "question_column_name = \"question\" \n",
    "context_column_name = \"context\"\n",
    "answer_column_name = \"answers\"\n",
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # truncation과 padding(length가 짧을때만)을 통해 toknization을 진행하며, stride를 이용하여 overflow를 유지합니다.\n",
    "    # 각 example들은 이전의 context와 조금씩 겹치게됩니다.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        #return_token_type_ids=False, # roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\n",
    "        padding=True,\n",
    "    )\n",
    "    tokenized_questions = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        max_length=512,\n",
    "        pad_to_max_length=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # 길이가 긴 context가 등장할 경우 truncate를 진행해야하므로, 해당 데이터셋을 찾을 수 있도록 mapping 가능한 값이 필요합니다.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # token의 캐릭터 단위 position를 찾을 수 있도록 offset mapping을 사용합니다.\n",
    "    # start_positions과 end_positions을 찾는데 도움을 줄 수 있습니다.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    # 데이터셋에 \"start position\", \"enc position\" label을 부여합니다.\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    tokenized_examples[\"q_input_ids\"] =[]\n",
    "    tokenized_examples[\"q_token_type_ids\"] =[]\n",
    "    tokenized_examples[\"q_attention_mask\"] = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)  # cls index\n",
    "        # sequence id를 설정합니다 (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        # 하나의 example이 여러개의 span을 가질 수 있습니다.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        tokenized_examples[\"q_input_ids\"].append(tokenized_questions['input_ids'][sample_index])\n",
    "        tokenized_examples[\"q_token_type_ids\"].append(tokenized_questions['token_type_ids'][sample_index])\n",
    "        tokenized_examples[\"q_attention_mask\"].append(tokenized_questions['attention_mask'][sample_index])\n",
    "        # answer가 없을 경우 cls_index를 answer로 설정합니다(== example에서 정답이 없는 경우 존재할 수 있음).\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # text에서 정답의 Start/end character index\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            answer = answers['text'][0]\n",
    "            tokenized_answer = tokenizer.encode(answer)[1:-1]\n",
    "            # text에서 current span의 Start token index\n",
    "            token_start_index = 0\n",
    "            # text에서 current span의 End token index\n",
    "            token_end_index = len(input_ids) - 1\n",
    "\n",
    "            # 정답이 span을 벗어났는지 확인합니다(정답이 없는 경우 CLS index로 label되어있음).\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # token_start_index 및 token_end_index를 answer의 끝으로 이동합니다.\n",
    "                # Note: answer가 마지막 단어인 경우 last offset을 따라갈 수 있습니다(edge case).\n",
    "                while (\n",
    "                    token_start_index < len(offsets)\n",
    "                    and offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff761e6faaf44e1baafcbdbcb8a44931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets['train']\n",
    "train_dataset = train_dataset.map(\n",
    "            prepare_train_features,\n",
    "            batched=True,\n",
    "            remove_columns=column_names,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'end_positions', 'input_ids', 'q_attention_mask', 'q_input_ids', 'q_token_type_ids', 'start_positions', 'token_type_ids'],\n",
       "    num_rows: 13492\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['q_token_type_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=torch.randn([32,512,768])\n",
    "b=torch.randn([32,512,768])\n",
    "c=a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([[[1,2,3],[4,5,6],[7,8,9],[0,1,2]],\n",
    "                [[1,2,3],[4,5,6],[7,8,9],[0,1,2]]])\n",
    "\n",
    "b=torch.tensor([[[0,1,0],[1,1,1],[2,2,2],[-1,1,-1]],\n",
    "                [[1,2,3],[4,5,6],[7,8,9],[0,1,2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  2,  0],\n",
       "         [ 4,  5,  6],\n",
       "         [14, 16, 18],\n",
       "         [ 0,  1, -2]],\n",
       "\n",
       "        [[ 1,  4,  9],\n",
       "         [16, 25, 36],\n",
       "         [49, 64, 81],\n",
       "         [ 0,  1,  4]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
